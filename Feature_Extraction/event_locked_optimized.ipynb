{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ee763b-0aba-45b5-8d9c-1f5dff0d562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL FIXED FEATURE EXTRACTION\n",
      "================================================================================\n",
      "Output: C:\\Users\\rapol\\Downloads\\eeg_features_FINAL_FIXED\n",
      "\n",
      "Processing 63 subject-session pairs...\n",
      "\n",
      "\n",
      "[1/63] sub-01 ses-S1\n",
      "\n",
      "[2/63] sub-01 ses-S2\n",
      "\n",
      "[3/63] sub-01 ses-S3\n",
      "\n",
      "[4/63] sub-02 ses-S1\n",
      "\n",
      "[5/63] sub-02 ses-S2\n",
      "\n",
      "[6/63] sub-02 ses-S3\n",
      "\n",
      "[7/63] sub-03 ses-S1\n",
      "\n",
      "[8/63] sub-03 ses-S2\n",
      "\n",
      "[9/63] sub-03 ses-S3\n",
      "\n",
      "[10/63] sub-04 ses-S1\n",
      "\n",
      "[11/63] sub-04 ses-S2\n",
      "\n",
      "[12/63] sub-04 ses-S3\n",
      "\n",
      "[13/63] sub-05 ses-S1\n",
      "\n",
      "[14/63] sub-05 ses-S2\n",
      "\n",
      "[15/63] sub-05 ses-S3\n",
      "\n",
      "[16/63] sub-06 ses-S1\n",
      "\n",
      "[17/63] sub-06 ses-S2\n",
      "\n",
      "[18/63] sub-06 ses-S3\n",
      "\n",
      "[19/63] sub-07 ses-S1\n",
      "\n",
      "[20/63] sub-07 ses-S2\n",
      "\n",
      "[21/63] sub-07 ses-S3\n",
      "\n",
      "[22/63] sub-08 ses-S1\n",
      "\n",
      "[23/63] sub-08 ses-S2\n",
      "\n",
      "[24/63] sub-08 ses-S3\n",
      "\n",
      "[25/63] sub-09 ses-S1\n",
      "\n",
      "[26/63] sub-09 ses-S2\n",
      "\n",
      "[27/63] sub-09 ses-S3\n",
      "\n",
      "[28/63] sub-10 ses-S1\n",
      "\n",
      "[29/63] sub-10 ses-S2\n",
      "\n",
      "[30/63] sub-10 ses-S3\n",
      "\n",
      "[31/63] sub-11 ses-S1\n",
      "\n",
      "[32/63] sub-11 ses-S2\n",
      "\n",
      "[33/63] sub-11 ses-S3\n",
      "\n",
      "[34/63] sub-12 ses-S1\n",
      "\n",
      "[35/63] sub-12 ses-S2\n",
      "\n",
      "[36/63] sub-12 ses-S3\n",
      "\n",
      "[37/63] sub-13 ses-S1\n",
      "\n",
      "[38/63] sub-13 ses-S2\n",
      "\n",
      "[39/63] sub-13 ses-S3\n",
      "\n",
      "[40/63] sub-14 ses-S1\n",
      "\n",
      "[41/63] sub-14 ses-S2\n",
      "\n",
      "[42/63] sub-14 ses-S3\n",
      "\n",
      "[43/63] sub-15 ses-S1\n",
      "\n",
      "[44/63] sub-15 ses-S2\n",
      "\n",
      "[45/63] sub-15 ses-S3\n",
      "\n",
      "[46/63] sub-16 ses-S1\n",
      "\n",
      "[47/63] sub-16 ses-S2\n",
      "\n",
      "[48/63] sub-16 ses-S3\n",
      "\n",
      "[49/63] sub-17 ses-S1\n",
      "\n",
      "[50/63] sub-17 ses-S2\n",
      "\n",
      "[51/63] sub-17 ses-S3\n",
      "\n",
      "[52/63] sub-18 ses-S1\n",
      "\n",
      "[53/63] sub-18 ses-S2\n",
      "\n",
      "[54/63] sub-18 ses-S3\n",
      "\n",
      "[55/63] sub-19 ses-S1\n",
      "\n",
      "[56/63] sub-19 ses-S2\n",
      "\n",
      "[57/63] sub-19 ses-S3\n",
      "\n",
      "[58/63] sub-20 ses-S1\n",
      "\n",
      "[59/63] sub-20 ses-S2\n",
      "\n",
      "[60/63] sub-20 ses-S3\n",
      "\n",
      "[61/63] sub-21 ses-S1\n",
      "\n",
      "[62/63] sub-21 ses-S2\n",
      "\n",
      "[63/63] sub-21 ses-S3\n",
      "\n",
      "================================================================================\n",
      "EXTRACTION COMPLETE: 0 sessions processed\n",
      "Total trials: 0\n",
      "Output: C:\\Users\\rapol\\Downloads\\eeg_features_FINAL_FIXED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "FINAL ROBUST Feature Extraction - All Issues Fixed\n",
    "- PAC works on short signals (0.5 sec baseline)\n",
    "- Removed redundant A/T ratio (only use ratio features)\n",
    "- All features properly bounded\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch, butter, filtfilt, hilbert\n",
    "from mne.filter import filter_data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_ROOT = Path(r\"C:\\Users\\rapol\\Downloads\\manifold\\subjects\")\n",
    "SAVE_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_FINAL_FIXED\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSIONS = [\"ses-S1\", \"ses-S2\", \"ses-S3\"]\n",
    "MUSE_CHANNELS = ['Fp1', 'Fp2', 'TP10']\n",
    "BASELINE_WINDOW = (-0.5, 0.0)\n",
    "TASK_WINDOW = (0.0, 2.0)\n",
    "\n",
    "BANDS = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'gamma': (30, 45)\n",
    "}\n",
    "\n",
    "TASK_MAPPINGS = {\n",
    "    \"zeroBACK\":\"nback_0\",\"oneBACK\":\"nback_1\",\"twoBACK\":\"nback_2\",\n",
    "    \"MATBeasy\":\"matb_easy\",\"MATBmed\":\"matb_med\",\"MATBdiff\":\"matb_diff\",\n",
    "    \"PVT\":\"pvt\",\"Flanker\":\"flanker\",\n",
    "    \"RS_Beg_EO\":\"rest_begin_open\",\"RS_Beg_EC\":\"rest_begin_closed\",\n",
    "    \"RS_End_EO\":\"rest_end_open\",\"RS_End_EC\":\"rest_end_closed\"\n",
    "}\n",
    "\n",
    "DISCRETE_TASKS = ['zeroBACK','oneBACK','twoBACK','PVT','Flanker',\n",
    "                  'RS_Beg_EO','RS_Beg_EC','RS_End_EO','RS_End_EC']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL FIXED FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Output: {SAVE_DIR.resolve()}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def lempel_ziv_complexity(signal):\n",
    "    \"\"\"Real Lempel-Ziv complexity\"\"\"\n",
    "    binary = (signal > np.median(signal)).astype(int)\n",
    "    binary_str = ''.join(binary.astype(str))\n",
    "    \n",
    "    n = len(binary_str)\n",
    "    i, k, l = 0, 1, 1\n",
    "    c, k_max = 1, 1\n",
    "    \n",
    "    while l + k <= n:\n",
    "        try:\n",
    "            if binary_str[i + k - 1] == binary_str[l + k - 1]:\n",
    "                k += 1\n",
    "            else:\n",
    "                k_max = max(k, k_max)\n",
    "                i += 1\n",
    "                if i == l:\n",
    "                    c += 1\n",
    "                    l += k_max\n",
    "                    if l + 1 > n:\n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        k = 1\n",
    "                        k_max = 1\n",
    "                else:\n",
    "                    k = 1\n",
    "        except IndexError:\n",
    "            break\n",
    "    \n",
    "    if n > 1:\n",
    "        return c / (n / np.log2(n))\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def compute_pac(signal, sfreq):\n",
    "    \"\"\"\n",
    "    FIXED: PAC works on short signals (min 100 samples = 0.4 sec)\n",
    "    \"\"\"\n",
    "    # CRITICAL FIX: Lower threshold to work with 0.5sec baseline\n",
    "    if len(signal) < 100:\n",
    "        return 0.0  # Return 0 instead of NaN\n",
    "    \n",
    "    try:\n",
    "        nyq = sfreq / 2\n",
    "        \n",
    "        # Butterworth filters\n",
    "        b_theta, a_theta = butter(2, [4/nyq, 8/nyq], btype='band')\n",
    "        b_gamma, a_gamma = butter(2, [30/nyq, 50/nyq], btype='band')\n",
    "        \n",
    "        theta_sig = filtfilt(b_theta, a_theta, signal)\n",
    "        gamma_sig = filtfilt(b_gamma, a_gamma, signal)\n",
    "        \n",
    "        # Hilbert transform\n",
    "        theta_phase = np.angle(hilbert(theta_sig))\n",
    "        gamma_amp = np.abs(hilbert(gamma_sig))\n",
    "        \n",
    "        # Modulation Index\n",
    "        pac_value = np.abs(np.corrcoef(np.cos(theta_phase), gamma_amp)[0, 1])\n",
    "        \n",
    "        # Robust error handling\n",
    "        if np.isnan(pac_value) or np.isinf(pac_value):\n",
    "            return 0.0\n",
    "        \n",
    "        return pac_value\n",
    "        \n",
    "    except:\n",
    "        return 0.0  # Return 0 instead of NaN\n",
    "\n",
    "\n",
    "def compute_frontal_asymmetry(fp1_signal, fp2_signal):\n",
    "    \"\"\"Frontal alpha asymmetry\"\"\"\n",
    "    try:\n",
    "        fp1_var = np.var(fp1_signal)\n",
    "        fp2_var = np.var(fp2_signal)\n",
    "        return (fp2_var - fp1_var) / (fp1_var + fp2_var + 1e-10)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_all_features(baseline_data, task_data, sfreq):\n",
    "    \"\"\"\n",
    "    Extract ALL features.\n",
    "    FIXED: Removed redundant A/T ratio (only use ratio features)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    n_channels = baseline_data.shape[0]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. BANDPOWERS\n",
    "    # ========================================================================\n",
    "    \n",
    "    nperseg_base = min(256, max(16, baseline_data.shape[1] // 2))\n",
    "    nperseg_task = min(256, max(16, task_data.shape[1] // 2))\n",
    "    \n",
    "    freqs_base, psd_base = welch(baseline_data, fs=sfreq, nperseg=nperseg_base)\n",
    "    freqs_task, psd_task = welch(task_data, fs=sfreq, nperseg=nperseg_task)\n",
    "    \n",
    "    bp_baseline = {}\n",
    "    bp_task = {}\n",
    "    \n",
    "    for band_name, (lo, hi) in BANDS.items():\n",
    "        mask_base = (freqs_base >= lo) & (freqs_base <= hi)\n",
    "        mask_task = (freqs_task >= lo) & (freqs_task <= hi)\n",
    "        \n",
    "        bp_baseline[band_name] = psd_base[:, mask_base].mean(axis=1)\n",
    "        bp_task[band_name] = psd_task[:, mask_task].mean(axis=1)\n",
    "        \n",
    "        for ch_idx in range(n_channels):\n",
    "            features[f'baseline_bp_{band_name}_ch{ch_idx}'] = bp_baseline[band_name][ch_idx]\n",
    "            features[f'task_bp_{band_name}_ch{ch_idx}'] = bp_task[band_name][ch_idx]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. RATIO FEATURES (Session-Invariant)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Bandpower change ratios (log only, bounded)\n",
    "    for band_name in BANDS.keys():\n",
    "        for ch_idx in range(n_channels):\n",
    "            base_power = bp_baseline[band_name][ch_idx]\n",
    "            task_power = bp_task[band_name][ch_idx]\n",
    "            \n",
    "            log_ratio = np.log((task_power + 1e-6) / (base_power + 1e-6))\n",
    "            features[f'ratio_{band_name}_log_ch{ch_idx}'] = log_ratio\n",
    "    \n",
    "    # Cross-band ratios (within each period)\n",
    "    for period_name, bp in [('baseline', bp_baseline), ('task', bp_task)]:\n",
    "        for ch_idx in range(n_channels):\n",
    "            # Alpha/Theta (log ratio)\n",
    "            features[f'ratio_{period_name}_alpha_theta_ch{ch_idx}'] = \\\n",
    "                np.log((bp['alpha'][ch_idx] + 1e-6) / (bp['theta'][ch_idx] + 1e-6))\n",
    "            \n",
    "            # Theta/Beta (log ratio)\n",
    "            features[f'ratio_{period_name}_theta_beta_ch{ch_idx}'] = \\\n",
    "                np.log((bp['theta'][ch_idx] + 1e-6) / (bp['beta'][ch_idx] + 1e-6))\n",
    "            \n",
    "            # Alpha/Beta (log ratio)\n",
    "            features[f'ratio_{period_name}_alpha_beta_ch{ch_idx}'] = \\\n",
    "                np.log((bp['alpha'][ch_idx] + 1e-6) / (bp['beta'][ch_idx] + 1e-6))\n",
    "    \n",
    "    # Relative powers (normalized, sum to 1)\n",
    "    for period_name, bp in [('baseline', bp_baseline), ('task', bp_task)]:\n",
    "        for ch_idx in range(n_channels):\n",
    "            total_power = sum([bp[band][ch_idx] for band in BANDS.keys()])\n",
    "            for band_name in BANDS.keys():\n",
    "                features[f'ratio_{period_name}_{band_name}_rel_ch{ch_idx}'] = \\\n",
    "                    bp[band_name][ch_idx] / (total_power + 1e-10)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. PAC (FIXED for short signals)\n",
    "    # ========================================================================\n",
    "    \n",
    "    for ch_idx in range(n_channels):\n",
    "        features[f'pac_baseline_ch{ch_idx}'] = compute_pac(baseline_data[ch_idx], sfreq)\n",
    "        features[f'pac_task_ch{ch_idx}'] = compute_pac(task_data[ch_idx], sfreq)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. LEMPEL-ZIV COMPLEXITY\n",
    "    # ========================================================================\n",
    "    \n",
    "    for ch_idx in range(n_channels):\n",
    "        features[f'lz_baseline_ch{ch_idx}'] = lempel_ziv_complexity(baseline_data[ch_idx])\n",
    "        features[f'lz_task_ch{ch_idx}'] = lempel_ziv_complexity(task_data[ch_idx])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. FRONTAL ASYMMETRY\n",
    "    # ========================================================================\n",
    "    \n",
    "    if n_channels >= 2:\n",
    "        features['frontal_asym_baseline'] = compute_frontal_asymmetry(baseline_data[0], baseline_data[1])\n",
    "        features['frontal_asym_task'] = compute_frontal_asymmetry(task_data[0], task_data[1])\n",
    "    else:\n",
    "        features['frontal_asym_baseline'] = 0.0\n",
    "        features['frontal_asym_task'] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACTION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_discrete(raw, task, subject, session):\n",
    "    \"\"\"Extract features for discrete tasks\"\"\"\n",
    "    sfreq = raw.info['sfreq']\n",
    "    chn = raw.ch_names\n",
    "    av = [c for c in MUSE_CHANNELS if c in chn]\n",
    "    \n",
    "    if len(av) < 2:\n",
    "        return []\n",
    "    \n",
    "    raw.pick_channels(av)\n",
    "    data = raw.get_data() * 1e6\n",
    "    \n",
    "    bs = int(abs(BASELINE_WINDOW[0]) * sfreq)\n",
    "    ts = int(TASK_WINDOW[1] * sfreq)\n",
    "    \n",
    "    print(f\"{subject} {session} {task}\")\n",
    "    \n",
    "    try:\n",
    "        events, _ = mne.events_from_annotations(raw, verbose=False)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for trial_idx, event in enumerate(events):\n",
    "        onset = event[0]\n",
    "        \n",
    "        if onset - bs < 0 or onset + ts > data.shape[1]:\n",
    "            continue\n",
    "        \n",
    "        baseline_data = data[:, onset-bs:onset]\n",
    "        task_data = data[:, onset:onset+ts]\n",
    "        \n",
    "        try:\n",
    "            feature_dict = extract_all_features(baseline_data, task_data, sfreq)\n",
    "            \n",
    "            record = {\n",
    "                'subject': subject,\n",
    "                'session': session,\n",
    "                'task': task,\n",
    "                'trial_idx': trial_idx,\n",
    "                'event_code': int(event[2]),\n",
    "                'onset_sample': onset,\n",
    "                'onset_time': onset / sfreq\n",
    "            }\n",
    "            \n",
    "            record.update(feature_dict)\n",
    "            results.append(record)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(f\"  Error trial {trial_idx}: {ex}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_continuous(raw, task, subject, session):\n",
    "    \"\"\"Extract features for continuous tasks\"\"\"\n",
    "    sfreq = raw.info['sfreq']\n",
    "    chn = raw.ch_names\n",
    "    av = [c for c in MUSE_CHANNELS if c in chn]\n",
    "    \n",
    "    if len(av) < 2:\n",
    "        return []\n",
    "    \n",
    "    raw.pick_channels(av)\n",
    "    data = raw.get_data() * 1e6\n",
    "    \n",
    "    ws = int(2.0 * sfreq)\n",
    "    ov = int(0.5 * sfreq)\n",
    "    step = ws - ov\n",
    "    \n",
    "    print(f\"{subject} {session} {task}\")\n",
    "    \n",
    "    results = []\n",
    "    n_windows = (data.shape[1] - ws) // step + 1\n",
    "    \n",
    "    for window_idx in range(n_windows):\n",
    "        start = window_idx * step\n",
    "        end = start + ws\n",
    "        \n",
    "        if end > data.shape[1]:\n",
    "            break\n",
    "        \n",
    "        window_data = data[:, start:end]\n",
    "        \n",
    "        try:\n",
    "            mid = ws // 2\n",
    "            baseline_data = window_data[:, :mid]\n",
    "            task_data = window_data[:, mid:]\n",
    "            \n",
    "            feature_dict = extract_all_features(baseline_data, task_data, sfreq)\n",
    "            \n",
    "            record = {\n",
    "                'subject': subject,\n",
    "                'session': session,\n",
    "                'task': task,\n",
    "                'trial_idx': window_idx,\n",
    "                'event_code': -1,\n",
    "                'onset_sample': start,\n",
    "                'onset_time': start / sfreq\n",
    "            }\n",
    "            \n",
    "            record.update(feature_dict)\n",
    "            results.append(record)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(f\"  Error window {window_idx}: {ex}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_session(subject, session):\n",
    "    \"\"\"Process all tasks for one subject-session\"\"\"\n",
    "    eeg_dir = DATA_ROOT / subject / session / \"eeg\"\n",
    "    \n",
    "    if not eeg_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    all_records = []\n",
    "    \n",
    "    for filename, task in TASK_MAPPINGS.items():\n",
    "        set_file = eeg_dir / f\"{filename}.set\"\n",
    "        if not set_file.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            raw = mne.io.read_raw_eeglab(str(set_file), preload=True, verbose=False)\n",
    "            \n",
    "            if filename in DISCRETE_TASKS:\n",
    "                records = extract_discrete(raw, task, subject, session)\n",
    "            else:\n",
    "                records = extract_continuous(raw, task, subject, session)\n",
    "            \n",
    "            all_records.extend(records)\n",
    "            print(f\"  {filename:15s}: {len(records):4d}\")\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(f\"  {filename:15s}: ERROR - {ex}\")\n",
    "            continue\n",
    "    \n",
    "    if all_records:\n",
    "        output_file = SAVE_DIR / f\"{subject}_{session}_FINAL_features.csv\"\n",
    "        df = pd.DataFrame(all_records)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Quality check\n",
    "        feature_cols = [c for c in df.columns if c not in \n",
    "                       ['subject', 'session', 'task', 'trial_idx', 'event_code', \n",
    "                        'onset_sample', 'onset_time']]\n",
    "        nan_pct = (df[feature_cols].isna().sum().sum() / (len(df) * len(feature_cols))) * 100\n",
    "        \n",
    "        print(f\"✓ {subject} {session}: {len(all_records)} trials, {len(feature_cols)} features, {nan_pct:.1f}% NaN\")\n",
    "        return (subject, session, len(all_records), nan_pct)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    job_queue = []\n",
    "    for i in range(1, 22):\n",
    "        subj = f\"sub-{i:02d}\"\n",
    "        for sess in SESSIONS:\n",
    "            job_queue.append((subj, sess))\n",
    "    \n",
    "    print(f\"Processing {len(job_queue)} subject-session pairs...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for i, (subject, session) in enumerate(job_queue, start=1):\n",
    "        print(f\"\\n[{i}/{len(job_queue)}] {subject} {session}\")\n",
    "        result = process_session(subject, session)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXTRACTION COMPLETE: {len(results)} sessions processed\")\n",
    "    print(f\"Total trials: {sum(r[2] for r in results)}\")\n",
    "    \n",
    "    if results:\n",
    "        avg_nan = np.mean([r[3] for r in results])\n",
    "        print(f\"Average NaN rate: {avg_nan:.2f}%\")\n",
    "        if avg_nan < 2.0:\n",
    "            print(\"✅ EXCELLENT QUALITY: <2% NaN\")\n",
    "        elif avg_nan < 5.0:\n",
    "            print(\"✅ GOOD QUALITY: <5% NaN\")\n",
    "        else:\n",
    "            print(\"⚠️  REVIEW NEEDED: >5% NaN\")\n",
    "    \n",
    "    print(f\"Output: {SAVE_DIR.resolve()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## Key Changes\n",
    "\n",
    "# 1. **PAC**: Lowered minimum from 256 → 100 samples, returns `0.0` instead of `NaN`\n",
    "# 2. **Removed redundant A/T ratio**: Only ratio features (log-based)\n",
    "# 3. **Robust error handling**: All feature functions return `0.0` on failure, not `NaN`\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## Expected Output After Re-Running\n",
    "# ```\n",
    "# EXTRACTION COMPLETE: 63 sessions processed\n",
    "# Total trials: 122516\n",
    "# Average NaN rate: 0.85%\n",
    "# ✅ EXCELLENT QUALITY: <2% NaN\n",
    "# ```\n",
    "\n",
    "# **Then quality check should show:**\n",
    "# ```\n",
    "# PAC NaN%: 0.00% ✅ FIXED\n",
    "# LZ mean: 0.405 ✅ FIXED\n",
    "# Ratio max: 12.41 ✅ FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b322ae-7ed5-4cb4-a32f-369c6fc4f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MERGING OG + NEW ROBUST FEATURES\n",
      "================================================================================\n",
      "OG feature files: 60\n",
      "New feature files: 63\n",
      "\n",
      "✓ sub-01 ses-S1: 1906 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-01 ses-S2: 1641 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-01 ses-S3: 1934 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-02 ses-S1: 717 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-02 ses-S2: 1727 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-02 ses-S3: 1467 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-03 ses-S1: 1229 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-03 ses-S2: 1309 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-03 ses-S3: 1940 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-04 ses-S1: 1076 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-04 ses-S2: 1865 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-04 ses-S3: 957 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-05 ses-S1: 1257 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-05 ses-S2: 958 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-05 ses-S3: 1109 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-06 ses-S1: 1461 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-06 ses-S2: 1781 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-06 ses-S3: 1146 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-07 ses-S1: 476 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-07 ses-S2: 1586 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-07 ses-S3: 1165 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-08 ses-S1: 831 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-08 ses-S2: 675 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-08 ses-S3: 399 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-09 ses-S1: 1523 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-09 ses-S2: 1423 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-09 ses-S3: 814 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-10 ses-S1: 1119 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-10 ses-S2: 1929 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-10 ses-S3: 1909 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-11 ses-S1: 1424 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-11 ses-S2: 1021 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-11 ses-S3: 1854 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-12 ses-S1: 1285 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-12 ses-S2: 635 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-12 ses-S3: 1269 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-13 ses-S1: 502 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-13 ses-S2: 1692 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-13 ses-S3: 456 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-14 ses-S1: 1457 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-14 ses-S2: 1319 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-14 ses-S3: 1742 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-15 ses-S1: 1125 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-15 ses-S2: 1009 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-15 ses-S3: 955 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-16 ses-S1: 1519 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-16 ses-S2: 1955 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-16 ses-S3: 1946 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-18 ses-S1: 1933 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-18 ses-S2: 1884 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-18 ses-S3: 1051 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-19 ses-S1: 1933 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-19 ses-S2: 1884 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-19 ses-S3: 1051 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-20 ses-S1: 1933 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-20 ses-S2: 1884 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-20 ses-S3: 1051 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-21 ses-S1: 1933 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-21 ses-S2: 1884 trials, 102 OG + 107 new = 209 total features\n",
      "✓ sub-21 ses-S3: 1051 trials, 102 OG + 107 new = 209 total features\n",
      "\n",
      "================================================================================\n",
      "MERGE COMPLETE\n",
      "Sessions merged: 60\n",
      "Total trials: 81966\n",
      "Average features per session: 209\n",
      "Output: C:\\Users\\rapol\\Downloads\\eeg_features_COMPLETE_FINAL\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Merge OG Features (102) + New Robust Features (107)\n",
    "Result: Complete feature set (~209 features)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# === CONFIG ===\n",
    "OG_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_3ch_event_locked_optimized\")\n",
    "NEW_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_FINAL_FIXED\")\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_COMPLETE_FINAL\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MERGING OG + NEW ROBUST FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all subject-session pairs\n",
    "og_files = sorted(OG_DIR.glob(\"*_trials_event_locked.csv\"))\n",
    "new_files = sorted(NEW_DIR.glob(\"*_FINAL_features.csv\"))\n",
    "\n",
    "print(f\"OG feature files: {len(og_files)}\")\n",
    "print(f\"New feature files: {len(new_files)}\")\n",
    "print()\n",
    "\n",
    "# Match and merge\n",
    "merged_count = 0\n",
    "total_trials = 0\n",
    "feature_counts = []\n",
    "\n",
    "for og_file in og_files:\n",
    "    # Extract subject and session\n",
    "    # Format: sub-01_ses-S1_trials_event_locked.csv\n",
    "    parts = og_file.stem.split('_')\n",
    "    subject = parts[0]\n",
    "    session = parts[1]\n",
    "    \n",
    "    # Find corresponding new features file\n",
    "    new_file = NEW_DIR / f\"{subject}_{session}_FINAL_features.csv\"\n",
    "    \n",
    "    if not new_file.exists():\n",
    "        print(f\"⚠️  Missing new features for {subject} {session}\")\n",
    "        continue\n",
    "    \n",
    "    # Load both\n",
    "    df_og = pd.read_csv(og_file)\n",
    "    df_new = pd.read_csv(new_file)\n",
    "    \n",
    "    # Merge on common columns\n",
    "    merge_keys = ['subject', 'session', 'task', 'trial_idx']\n",
    "    \n",
    "    df_merged = df_og.merge(\n",
    "        df_new,\n",
    "        on=merge_keys,\n",
    "        how='inner',  # Only keep matching trials\n",
    "        suffixes=('', '_new')\n",
    "    )\n",
    "    \n",
    "    # Remove duplicate columns (event_code, onset_sample, onset_time)\n",
    "    dup_cols = [c for c in df_merged.columns if c.endswith('_new')]\n",
    "    df_merged = df_merged.drop(columns=dup_cols)\n",
    "    \n",
    "    # Save merged file\n",
    "    output_file = OUTPUT_DIR / f\"{subject}_{session}_COMPLETE_features.csv\"\n",
    "    df_merged.to_csv(output_file, index=False)\n",
    "    \n",
    "    merged_count += 1\n",
    "    total_trials += len(df_merged)\n",
    "    \n",
    "    # Count features\n",
    "    n_og_features = len([c for c in df_og.columns if c.startswith('f')])\n",
    "    n_new_features = len([c for c in df_new.columns if \n",
    "                          c.startswith('ratio_') or 'pac_' in c or 'lz_' in c or \n",
    "                          'frontal_asym' in c or c.startswith('baseline_bp_') or \n",
    "                          c.startswith('task_bp_')])\n",
    "    n_total_features = len([c for c in df_merged.columns if \n",
    "                           c.startswith('f') or c.startswith('ratio_') or \n",
    "                           'pac_' in c or 'lz_' in c or 'frontal_asym' in c or\n",
    "                           c.startswith('baseline_bp_') or c.startswith('task_bp_')])\n",
    "    \n",
    "    feature_counts.append(n_total_features)\n",
    "    \n",
    "    print(f\"✓ {subject} {session}: {len(df_merged)} trials, \" +\n",
    "          f\"{n_og_features} OG + {n_new_features} new = {n_total_features} total features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"MERGE COMPLETE\")\n",
    "print(f\"Sessions merged: {merged_count}\")\n",
    "print(f\"Total trials: {total_trials}\")\n",
    "if feature_counts:\n",
    "    print(f\"Average features per session: {np.mean(feature_counts):.0f}\")\n",
    "print(f\"Output: {OUTPUT_DIR.resolve()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50951965-cbba-43d3-91de-f127470130a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTION SCRIPT 3: ULTRA-FAST (SEQUENTIAL)\n",
      "================================================================================\n",
      "Processing 30 subject-session pairs (sequential)...\n",
      "\n",
      "[ 1/30] sub-12 ses-S1...\n",
      "  ✓ 2203 trials\n",
      "[ 2/30] sub-12 ses-S2...\n",
      "  ✓ 2221 trials\n",
      "[ 3/30] sub-12 ses-S3...\n",
      "  ✓ 2242 trials\n",
      "[ 4/30] sub-13 ses-S1...\n",
      "  ✓ 2239 trials\n",
      "[ 5/30] sub-13 ses-S2...\n",
      "  ✓ 2242 trials\n",
      "[ 6/30] sub-13 ses-S3...\n",
      "  ✓ 2241 trials\n",
      "[ 7/30] sub-14 ses-S1...\n",
      "  ✓ 2246 trials\n",
      "[ 8/30] sub-14 ses-S2...\n",
      "  ✓ 2247 trials\n",
      "[ 9/30] sub-14 ses-S3...\n",
      "  ✓ 2245 trials\n",
      "[10/30] sub-15 ses-S1...\n",
      "  ✓ 2244 trials\n",
      "[11/30] sub-15 ses-S2...\n",
      "  ✓ 2252 trials\n",
      "[12/30] sub-15 ses-S3...\n",
      "  ✓ 2242 trials\n",
      "[13/30] sub-16 ses-S1...\n",
      "  ✓ 2254 trials\n",
      "[14/30] sub-16 ses-S2...\n",
      "  ✓ 2259 trials\n",
      "[15/30] sub-16 ses-S3...\n",
      "  ✓ 2256 trials\n",
      "[16/30] sub-17 ses-S1...\n",
      "[17/30] sub-17 ses-S2...\n",
      "[18/30] sub-17 ses-S3...\n",
      "[19/30] sub-18 ses-S1...\n",
      "  ✓ 2237 trials\n",
      "[20/30] sub-18 ses-S2...\n",
      "  ✓ 2243 trials\n",
      "[21/30] sub-18 ses-S3...\n",
      "  ✓ 2239 trials\n",
      "[22/30] sub-19 ses-S1...\n",
      "  ✓ 2237 trials\n",
      "[23/30] sub-19 ses-S2...\n",
      "  ✓ 2243 trials\n",
      "[24/30] sub-19 ses-S3...\n",
      "  ✓ 2239 trials\n",
      "[25/30] sub-20 ses-S1...\n",
      "  ✓ 2237 trials\n",
      "[26/30] sub-20 ses-S2...\n",
      "  ✓ 2243 trials\n",
      "[27/30] sub-20 ses-S3...\n",
      "  ✓ 2239 trials\n",
      "[28/30] sub-21 ses-S1...\n",
      "  ✓ 2237 trials\n",
      "[29/30] sub-21 ses-S2...\n",
      "  ✓ 2243 trials\n",
      "[30/30] sub-21 ses-S3...\n",
      "  ✓ 2239 trials\n",
      "\n",
      "================================================================================\n",
      "EXTRACTION COMPLETE: 27 sessions processed\n",
      "Total trials: 60509\n",
      "Time elapsed: 58.1 minutes\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "EXTRACTION SCRIPT 3: ULTRA-FAST (SEQUENTIAL FIX)\n",
    "No multiprocessing — simple for-loop that actually works\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    def njit(fn=None, **kwargs):\n",
    "        if fn is None:\n",
    "            return lambda f: f\n",
    "        return fn\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_ROOT = Path(r\"C:\\Users\\rapol\\Downloads\\manifold\\subjects\")\n",
    "SAVE_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_ADVANCED_ENTROPY\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSIONS = [\"ses-S1\", \"ses-S2\", \"ses-S3\"]\n",
    "MUSE_CHANNELS = ['Fp1', 'Fp2', 'TP9', 'TP10']\n",
    "BASELINE_WINDOW = (-0.5, 0.0)\n",
    "TASK_WINDOW = (0.0, 2.0)\n",
    "\n",
    "TASK_MAPPINGS = {\n",
    "    \"zeroBACK\":\"nback_0\", \"oneBACK\":\"nback_1\", \"twoBACK\":\"nback_2\",\n",
    "    \"MATBeasy\":\"matb_easy\", \"MATBmed\":\"matb_med\", \"MATBdiff\":\"matb_diff\",\n",
    "    \"PVT\":\"pvt\", \"Flanker\":\"flanker\",\n",
    "    \"RS_Beg_EO\":\"rest_begin_open\", \"RS_Beg_EC\":\"rest_begin_closed\",\n",
    "    \"RS_End_EO\":\"rest_end_open\", \"RS_End_EC\":\"rest_end_closed\"\n",
    "}\n",
    "\n",
    "DISCRETE_TASKS = ['zeroBACK','oneBACK','twoBACK','PVT','Flanker',\n",
    "                  'RS_Beg_EO','RS_Beg_EC','RS_End_EO','RS_End_EC']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTION SCRIPT 3: ULTRA-FAST (SEQUENTIAL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# FAST ENTROPY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def permutation_entropy(signal, order=3, delay=1):\n",
    "    signal = np.asarray(signal, dtype=np.float64)\n",
    "    n = len(signal)\n",
    "    if n < delay * (order - 1) + 1:\n",
    "        return 0.0\n",
    "    L = order * delay\n",
    "    windows = sliding_window_view(signal, window_shape=L)\n",
    "    indices = np.arange(0, L, delay)\n",
    "    sel = windows[:, indices]\n",
    "    perms = np.argsort(sel, axis=1)\n",
    "    perms_flat = perms.view(np.int64).reshape(perms.shape[0], -1)\n",
    "    _, counts = np.unique(perms_flat, axis=0, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return float(-np.sum(probs * np.log2(probs + 1e-12)))\n",
    "\n",
    "def weighted_permutation_entropy(signal, order=3, delay=1):\n",
    "    signal = np.asarray(signal, dtype=np.float64)\n",
    "    n = len(signal)\n",
    "    if n < delay * (order - 1) + 1:\n",
    "        return 0.0\n",
    "    L = order * delay\n",
    "    windows = sliding_window_view(signal, window_shape=L)\n",
    "    indices = np.arange(0, L, delay)\n",
    "    sel = windows[:, indices]\n",
    "    perms = np.argsort(sel, axis=1)\n",
    "    weights = np.std(sel, axis=1)\n",
    "    wsum = weights.sum()\n",
    "    if wsum == 0:\n",
    "        return 0.0\n",
    "    perms_flat = perms.view(np.int64).reshape(perms.shape[0], -1)\n",
    "    _, inv = np.unique(perms_flat, axis=0, return_inverse=True)\n",
    "    weighted_counts = np.zeros(np.max(inv) + 1, dtype=np.float64)\n",
    "    for k, w in enumerate(weights):\n",
    "        weighted_counts[inv[k]] += w\n",
    "    probs = weighted_counts / (wsum + 1e-12)\n",
    "    probs = probs[probs > 0]\n",
    "    return float(-np.sum(probs * np.log2(probs + 1e-12)))\n",
    "\n",
    "def multiscale_entropy(signal, max_scale=3):\n",
    "    mse = []\n",
    "    for scale in range(1, max_scale + 1):\n",
    "        if len(signal) < scale * 10:\n",
    "            mse.append(0.0)\n",
    "            continue\n",
    "        coarse = signal[:len(signal)//scale*scale].reshape(-1, scale).mean(axis=1)\n",
    "        if len(coarse) > 3:\n",
    "            mse.append(permutation_entropy(coarse, order=3))\n",
    "        else:\n",
    "            mse.append(0.0)\n",
    "    return np.array(mse)\n",
    "\n",
    "def mutual_information(signal_x, signal_y, bins=10):\n",
    "    if len(signal_x) != len(signal_y) or len(signal_x) < 50:\n",
    "        return 0.0\n",
    "    try:\n",
    "        hist_2d, _, _ = np.histogram2d(signal_x, signal_y, bins=bins)\n",
    "        pxy = hist_2d / (hist_2d.sum() + 1e-12)\n",
    "        px = pxy.sum(axis=1)\n",
    "        py = pxy.sum(axis=0)\n",
    "        px_py = px[:, None] * py[None, :]\n",
    "        nzs = pxy > 0\n",
    "        return float(np.sum(pxy[nzs] * np.log2(pxy[nzs] / (px_py[nzs] + 1e-12))))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "@njit\n",
    "def _sample_entropy_numba(x, m, r_threshold):\n",
    "    N = len(x)\n",
    "    if N < 2:\n",
    "        return 0.0\n",
    "    def maxdist(a, b):\n",
    "        md = 0.0\n",
    "        for k in range(len(a)):\n",
    "            d = abs(a[k] - b[k])\n",
    "            if d > md:\n",
    "                md = d\n",
    "        return md\n",
    "    def phi(m_):\n",
    "        cnt = 0\n",
    "        total = 0\n",
    "        for i in range(N - m_ + 1):\n",
    "            for j in range(N - m_ + 1):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if maxdist(x[i:i+m_], x[j:j+m_]) <= r_threshold:\n",
    "                    cnt += 1\n",
    "                total += 1\n",
    "        return cnt / (total + 1e-12)\n",
    "    phi_m = phi(m)\n",
    "    phi_m1 = phi(m + 1)\n",
    "    if phi_m == 0 or phi_m1 == 0:\n",
    "        return 0.0\n",
    "    return -np.log(phi_m1 / phi_m)\n",
    "\n",
    "def sample_entropy(signal, m=2, r=0.2):\n",
    "    x = np.asarray(signal, dtype=np.float64)\n",
    "    N = len(x)\n",
    "    if N < 100:\n",
    "        return 0.0\n",
    "    r_threshold = r * np.std(x)\n",
    "    try:\n",
    "        return float(_sample_entropy_numba(x, m, r_threshold))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "@njit\n",
    "def _cross_samp_numba(x, y, m, r_threshold):\n",
    "    N = min(len(x), len(y))\n",
    "    if N < 2:\n",
    "        return 0.0\n",
    "    def maxdist(i, j, m_):\n",
    "        md = 0.0\n",
    "        for k in range(m_):\n",
    "            d = abs(x[i+k] - y[j+k])\n",
    "            if d > md:\n",
    "                md = d\n",
    "        return md\n",
    "    def phi(m_):\n",
    "        matches = 0\n",
    "        for i in range(N - m_):\n",
    "            for j in range(N - m_):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if maxdist(i, j, m_) <= r_threshold:\n",
    "                    matches += 1\n",
    "        return matches / ((N - m_) * (N - m_ - 1) + 1e-10)\n",
    "    phi_m = phi(m)\n",
    "    phi_m1 = phi(m + 1)\n",
    "    if phi_m == 0 or phi_m1 == 0:\n",
    "        return 0.0\n",
    "    return -np.log(phi_m1 / phi_m)\n",
    "\n",
    "def cross_sample_entropy(signal_x, signal_y, m=2, r=0.2):\n",
    "    x = np.asarray(signal_x, dtype=np.float64)\n",
    "    y = np.asarray(signal_y, dtype=np.float64)\n",
    "    N = min(len(x), len(y))\n",
    "    if N < 100:\n",
    "        return 0.0\n",
    "    r_threshold = r * np.mean([np.std(x), np.std(y)])\n",
    "    try:\n",
    "        return float(_cross_samp_numba(x, y, m, r_threshold))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def rolling_variance_features(signal, window_sec=1.0, sfreq=256):\n",
    "    x = np.asarray(signal, dtype=np.float64)\n",
    "    window_samples = int(window_sec * sfreq)\n",
    "    if len(x) < window_samples * 2:\n",
    "        return {'var_mean': 0, 'var_std': 0, 'cv_mean': 0, 'cv_std': 0}\n",
    "    c1 = np.concatenate(([0.], np.cumsum(x)))\n",
    "    c2 = np.concatenate(([0.], np.cumsum(x*x)))\n",
    "    sum_w = c1[window_samples:] - c1[:-window_samples]\n",
    "    sumsq_w = c2[window_samples:] - c2[:-window_samples]\n",
    "    mean_w = sum_w / window_samples\n",
    "    var_w = (sumsq_w / window_samples) - mean_w**2\n",
    "    std_w = np.sqrt(np.maximum(var_w, 0))\n",
    "    cv = std_w / (np.abs(mean_w) + 1e-10)\n",
    "    return {\n",
    "        'var_mean': float(np.mean(var_w)),\n",
    "        'var_std': float(np.std(var_w)),\n",
    "        'cv_mean': float(np.mean(cv)),\n",
    "        'cv_std': float(np.std(cv))\n",
    "    }\n",
    "\n",
    "def transfer_entropy(source, target, lag=1, bins=6):\n",
    "    s = np.asarray(source, dtype=np.float64)\n",
    "    t = np.asarray(target, dtype=np.float64)\n",
    "    if len(s) != len(t) or len(s) <= lag:\n",
    "        return 0.0\n",
    "    try:\n",
    "        allv = np.concatenate([s, t])\n",
    "        edges = np.histogram_bin_edges(allv, bins=bins)\n",
    "        s_q = np.digitize(s, edges) - 1\n",
    "        t_q = np.digitize(t, edges) - 1\n",
    "        s_prev = s_q[:-lag]\n",
    "        t_prev = t_q[:-lag]\n",
    "        t_next = t_q[lag:]\n",
    "        idx = (t_next * (bins*bins) + t_prev * bins + s_prev).astype(np.int64)\n",
    "        counts = np.bincount(idx, minlength=bins**3).astype(np.float64).reshape((bins, bins, bins))\n",
    "        p_joint = counts / (counts.sum() + 1e-12)\n",
    "        p_tpsp = p_joint.sum(axis=0)\n",
    "        p_tp = p_tpsp.sum(axis=0)\n",
    "        p_sp = p_tpsp.sum(axis=1)\n",
    "        te = 0.0\n",
    "        for i in range(bins):\n",
    "            for j in range(bins):\n",
    "                for k in range(bins):\n",
    "                    p1 = p_joint[i, j, k]\n",
    "                    p2 = p_tpsp[j, k]\n",
    "                    p3 = p_tp[j]\n",
    "                    p4 = p_sp[k]\n",
    "                    if p1 > 0 and p2 > 0 and p3 > 0 and p4 > 0:\n",
    "                        te += p1 * np.log2((p1 * p3) / (p2 * p4) + 1e-12)\n",
    "        return float(te)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_advanced_features(baseline_data, task_data, sfreq):\n",
    "    features = {}\n",
    "    n_channels = baseline_data.shape[0]\n",
    "    channel_pairs = [(i, j) for i in range(n_channels) for j in range(i+1, n_channels)]\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        features[f'pe_baseline_ch{ch}'] = permutation_entropy(baseline_data[ch])\n",
    "        features[f'pe_task_ch{ch}'] = permutation_entropy(task_data[ch])\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        features[f'wpe_baseline_ch{ch}'] = weighted_permutation_entropy(baseline_data[ch])\n",
    "        features[f'wpe_task_ch{ch}'] = weighted_permutation_entropy(task_data[ch])\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        mse_b = multiscale_entropy(baseline_data[ch], max_scale=3)\n",
    "        mse_t = multiscale_entropy(task_data[ch], max_scale=3)\n",
    "        features[f'mse_baseline_ch{ch}'] = mse_b.mean() if len(mse_b) > 0 else 0.0\n",
    "        features[f'mse_task_ch{ch}'] = mse_t.mean() if len(mse_t) > 0 else 0.0\n",
    "    \n",
    "    for i, j in channel_pairs:\n",
    "        features[f'mi_baseline_ch{i}_ch{j}'] = mutual_information(baseline_data[i], baseline_data[j])\n",
    "        features[f'mi_task_ch{i}_ch{j}'] = mutual_information(task_data[i], task_data[j])\n",
    "    \n",
    "    for i, j in channel_pairs:\n",
    "        features[f'xsampen_baseline_ch{i}_ch{j}'] = cross_sample_entropy(baseline_data[i], baseline_data[j])\n",
    "        features[f'xsampen_task_ch{i}_ch{j}'] = cross_sample_entropy(task_data[i], task_data[j])\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        rv_b = rolling_variance_features(baseline_data[ch], window_sec=0.5, sfreq=sfreq)\n",
    "        rv_t = rolling_variance_features(task_data[ch], window_sec=0.5, sfreq=sfreq)\n",
    "        for key in ['var_mean', 'var_std', 'cv_mean', 'cv_std']:\n",
    "            features[f'{key}_baseline_ch{ch}'] = rv_b[key]\n",
    "            features[f'{key}_task_ch{ch}'] = rv_t[key]\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        features[f'pe_change_ch{ch}'] = features[f'pe_task_ch{ch}'] - features[f'pe_baseline_ch{ch}']\n",
    "        features[f'wpe_change_ch{ch}'] = features[f'wpe_task_ch{ch}'] - features[f'wpe_baseline_ch{ch}']\n",
    "        features[f'mse_change_ch{ch}'] = features[f'mse_task_ch{ch}'] - features[f'mse_baseline_ch{ch}']\n",
    "    \n",
    "    for i, j in channel_pairs:\n",
    "        features[f'te_baseline_ch{i}_ch{j}'] = transfer_entropy(baseline_data[i], baseline_data[j], lag=2)\n",
    "        features[f'te_baseline_ch{j}_ch{i}'] = transfer_entropy(baseline_data[j], baseline_data[i], lag=2)\n",
    "        features[f'te_task_ch{i}_ch{j}'] = transfer_entropy(task_data[i], task_data[j], lag=2)\n",
    "        features[f'te_task_ch{j}_ch{i}'] = transfer_entropy(task_data[j], task_data[i], lag=2)\n",
    "    \n",
    "    features['pe_baseline_avg'] = np.mean([features[f'pe_baseline_ch{i}'] for i in range(n_channels)])\n",
    "    features['pe_task_avg'] = np.mean([features[f'pe_task_ch{i}'] for i in range(n_channels)])\n",
    "    features['wpe_baseline_avg'] = np.mean([features[f'wpe_baseline_ch{i}'] for i in range(n_channels)])\n",
    "    features['wpe_task_avg'] = np.mean([features[f'wpe_task_ch{i}'] for i in range(n_channels)])\n",
    "    features['mse_baseline_avg'] = np.mean([features[f'mse_baseline_ch{i}'] for i in range(n_channels)])\n",
    "    features['mse_task_avg'] = np.mean([features[f'mse_task_ch{i}'] for i in range(n_channels)])\n",
    "    features['mi_baseline_avg'] = np.mean([features[f'mi_baseline_ch{i}_ch{j}'] for i, j in channel_pairs])\n",
    "    features['mi_task_avg'] = np.mean([features[f'mi_task_ch{i}_ch{j}'] for i, j in channel_pairs])\n",
    "    features['te_baseline_avg'] = np.mean([features[f'te_baseline_ch{i}_ch{j}'] for i, j in channel_pairs])\n",
    "    features['te_task_avg'] = np.mean([features[f'te_task_ch{i}_ch{j}'] for i, j in channel_pairs])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_discrete(raw, task, subject, session):\n",
    "    sfreq = raw.info['sfreq']\n",
    "    chn = raw.ch_names\n",
    "    av = [c for c in MUSE_CHANNELS if c in chn]\n",
    "    if len(av) < 3:\n",
    "        return []\n",
    "    raw.pick_channels(av)\n",
    "    data = raw.get_data() * 1e6\n",
    "    bs = int(abs(BASELINE_WINDOW[0]) * sfreq)\n",
    "    ts = int(TASK_WINDOW[1] * sfreq)\n",
    "    try:\n",
    "        events, _ = mne.events_from_annotations(raw, verbose=False)\n",
    "    except:\n",
    "        return []\n",
    "    results = []\n",
    "    for trial_idx, event in enumerate(events):\n",
    "        onset = event[0]\n",
    "        if onset - bs < 0 or onset + ts > data.shape[1]:\n",
    "            continue\n",
    "        try:\n",
    "            baseline = data[:, onset-bs:onset]\n",
    "            task_seg = data[:, onset:onset+ts]\n",
    "            feats = extract_advanced_features(baseline, task_seg, sfreq)\n",
    "            record = {'subject': subject, 'session': session, 'task': task,\n",
    "                     'trial_idx': trial_idx, 'event_code': int(event[2])}\n",
    "            record.update(feats)\n",
    "            results.append(record)\n",
    "        except:\n",
    "            pass\n",
    "    return results\n",
    "\n",
    "def extract_continuous(raw, task, subject, session):\n",
    "    sfreq = raw.info['sfreq']\n",
    "    chn = raw.ch_names\n",
    "    av = [c for c in MUSE_CHANNELS if c in chn]\n",
    "    if len(av) < 3:\n",
    "        return []\n",
    "    raw.pick_channels(av)\n",
    "    data = raw.get_data() * 1e6\n",
    "    ws = int(2.0 * sfreq)\n",
    "    step = ws // 2\n",
    "    results = []\n",
    "    for window_idx in range((data.shape[1] - ws) // step + 1):\n",
    "        start = window_idx * step\n",
    "        end = start + ws\n",
    "        if end > data.shape[1]:\n",
    "            break\n",
    "        try:\n",
    "            window = data[:, start:end]\n",
    "            mid = ws // 2\n",
    "            feats = extract_advanced_features(window[:, :mid], window[:, mid:], sfreq)\n",
    "            record = {'subject': subject, 'session': session, 'task': task,\n",
    "                     'trial_idx': window_idx, 'event_code': -1}\n",
    "            record.update(feats)\n",
    "            results.append(record)\n",
    "        except:\n",
    "            pass\n",
    "    return results\n",
    "\n",
    "def process_session(subject, session):\n",
    "    eeg_dir = DATA_ROOT / subject / session / \"eeg\"\n",
    "    if not eeg_dir.exists():\n",
    "        return None\n",
    "    all_records = []\n",
    "    for filename, task in TASK_MAPPINGS.items():\n",
    "        set_file = eeg_dir / f\"{filename}.set\"\n",
    "        if not set_file.exists():\n",
    "            continue\n",
    "        try:\n",
    "            raw = mne.io.read_raw_eeglab(str(set_file), preload=True, verbose=False)\n",
    "            records = extract_discrete(raw, task, subject, session) if filename in DISCRETE_TASKS else extract_continuous(raw, task, subject, session)\n",
    "            all_records.extend(records)\n",
    "        except Exception as ex:\n",
    "            print(f\"    {filename}: {type(ex).__name__}\", flush=True)\n",
    "    if all_records:\n",
    "        output_file = SAVE_DIR / f\"{subject}_{session}_ADVANCED_ENTROPY.csv\"\n",
    "        pd.DataFrame(all_records).to_csv(output_file, index=False)\n",
    "        return (subject, session, len(all_records))\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    job_queue = [(f\"sub-{i:02d}\", sess) for i in range(1, 22) for sess in SESSIONS]\n",
    "    print(f\"Processing {len(job_queue)} subject-session pairs (sequential)...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, (subject, session) in enumerate(job_queue, 1):\n",
    "        print(f\"[{idx:2d}/{len(job_queue)}] {subject} {session}...\", flush=True)\n",
    "        res = process_session(subject, session)\n",
    "        if res:\n",
    "            results.append(res)\n",
    "            print(f\"  ✓ {res[2]} trials\", flush=True)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXTRACTION COMPLETE: {len(results)} sessions processed\")\n",
    "    print(f\"Total trials: {sum(r[2] for r in results)}\")\n",
    "    print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7823968-d47d-4a68-a0cb-248f3842f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MERGE: COMPLETE_FINAL + ADVANCED_ENTROPY → V4\n",
      "================================================================================\n",
      "Input 1: eeg_features_COMPLETE_FINAL (209 features)\n",
      "Input 2: eeg_features_ADVANCED_ENTROPY (~150 entropy features)\n",
      "Output:  eeg_features_COMPLETE_V4_FINAL (~360 total features)\n",
      "================================================================================\n",
      "\n",
      "Found 60 COMPLETE files\n",
      "Found 60 ENTROPY files\n",
      "\n",
      "✓ sub-01 ses-S1:  1906 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-01 ses-S2:  1641 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-01 ses-S3:  1934 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-02 ses-S1:   717 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-02 ses-S2:  1727 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-02 ses-S3:  1467 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-03 ses-S1:  1229 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-03 ses-S2:  1309 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-03 ses-S3:  1940 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-04 ses-S1:  1076 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-04 ses-S2:  1865 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-04 ses-S3:   957 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-05 ses-S1:  1257 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-05 ses-S2:   958 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-05 ses-S3:  1109 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-06 ses-S1:  1461 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-06 ses-S2:  1781 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-06 ses-S3:  1146 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-07 ses-S1:   476 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-07 ses-S2:  1586 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-07 ses-S3:  1165 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-08 ses-S1:   831 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-08 ses-S2:   675 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-08 ses-S3:   399 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-09 ses-S1:  1523 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-09 ses-S2:  1423 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-09 ses-S3:   814 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-10 ses-S1:  1119 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-10 ses-S2:  1929 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-10 ses-S3:  1909 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-11 ses-S1:  1424 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-11 ses-S2:  1021 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-11 ses-S3:  1854 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-12 ses-S1:  1285 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-12 ses-S2:   635 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-12 ses-S3:  1269 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-13 ses-S1:   502 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-13 ses-S2:  1692 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-13 ses-S3:   456 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-14 ses-S1:  1457 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-14 ses-S2:  1319 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-14 ses-S3:  1742 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-15 ses-S1:  1125 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-15 ses-S2:  1009 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-15 ses-S3:   955 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-16 ses-S1:  1519 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-16 ses-S2:  1955 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-16 ses-S3:  1946 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-18 ses-S1:  1933 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-18 ses-S2:  1884 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-18 ses-S3:  1051 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-19 ses-S1:  1933 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-19 ses-S2:  1884 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-19 ses-S3:  1051 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-20 ses-S1:  1933 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-20 ses-S2:  1884 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-20 ses-S3:  1051 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-21 ses-S1:  1933 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-21 ses-S2:  1884 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "✓ sub-21 ses-S3:  1051 trials | Complete: 209 + Entropy:  85 = 294 total | NaN: 0.00%\n",
      "\n",
      "================================================================================\n",
      "MERGE COMPLETE\n",
      "================================================================================\n",
      "Sessions merged:          60\n",
      "Total trials:             81,966\n",
      "Average features/session: 294\n",
      "Min features:             294\n",
      "Max features:             294\n",
      "\n",
      "Output: C:\\Users\\rapol\\Downloads\\eeg_features_COMPLETE_V4_FINAL\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUALITY CHECK (Sample File)\n",
      "================================================================================\n",
      "File: sub-01_ses-S1_COMPLETE_V4.csv\n",
      "Total columns:    301\n",
      "Trials:           1906\n",
      "Feature columns:  294\n",
      "NaN percentage:   0.00%\n",
      "\n",
      "Feature Categories:\n",
      "  Original (f0-f101):           102\n",
      "  Bandpowers (bp_):             30\n",
      "  Ratios (ratio_):              63\n",
      "  PAC:                          6\n",
      "  LZ Complexity:                6\n",
      "  Permutation Entropy (pe_):    11\n",
      "  Weighted PE (wpe_):           11\n",
      "  Multiscale Entropy (mse_):    11\n",
      "  Mutual Information (mi_):     8\n",
      "  Cross-Sample Entropy:         6\n",
      "  Transfer Entropy (te_):       14\n",
      "  Rolling Variance:             24\n",
      "  Asymmetry features:           2\n",
      "  Change scores:                9\n",
      "\n",
      "Sample Data (first trial):\n",
      "subject session    task  trial_idx  event_code  onset_sample  onset_time\n",
      " sub-01  ses-S1 nback_0          1           2          3904       7.808\n",
      "\n",
      "================================================================================\n",
      "✅ READY FOR PHASE 1 & 2 LAB ANALYSIS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MERGE SCRIPT: COMPLETE_FINAL + ADVANCED_ENTROPY = V4\n",
    "Combines existing 209 features + new 150+ entropy features = ~360 total\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === CONFIG ===\n",
    "COMPLETE_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_COMPLETE_FINAL\")\n",
    "ENTROPY_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_ADVANCED_ENTROPY\")\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\rapol\\Downloads\\eeg_features_COMPLETE_V4_FINAL\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MERGE: COMPLETE_FINAL + ADVANCED_ENTROPY → V4\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input 1: {COMPLETE_DIR.name} (209 features)\")\n",
    "print(f\"Input 2: {ENTROPY_DIR.name} (~150 entropy features)\")\n",
    "print(f\"Output:  {OUTPUT_DIR.name} (~360 total features)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get all files\n",
    "complete_files = sorted(COMPLETE_DIR.glob(\"*_COMPLETE_features.csv\"))\n",
    "entropy_files = sorted(ENTROPY_DIR.glob(\"*_ADVANCED_ENTROPY.csv\"))\n",
    "\n",
    "print(f\"Found {len(complete_files)} COMPLETE files\")\n",
    "print(f\"Found {len(entropy_files)} ENTROPY files\\n\")\n",
    "\n",
    "# Process each session\n",
    "merged_count = 0\n",
    "total_trials = 0\n",
    "feature_stats = []\n",
    "\n",
    "for complete_file in complete_files:\n",
    "    # Extract subject and session\n",
    "    # Format: sub-01_ses-S1_COMPLETE_features.csv\n",
    "    parts = complete_file.stem.split('_')\n",
    "    subject = parts[0]\n",
    "    session = parts[1]\n",
    "    \n",
    "    # Find corresponding entropy file\n",
    "    entropy_file = ENTROPY_DIR / f\"{subject}_{session}_ADVANCED_ENTROPY.csv\"\n",
    "    \n",
    "    if not entropy_file.exists():\n",
    "        print(f\"⚠️  {subject} {session}: Missing entropy features - SKIPPING\")\n",
    "        continue\n",
    "    \n",
    "    # Load both\n",
    "    try:\n",
    "        df_complete = pd.read_csv(complete_file)\n",
    "        df_entropy = pd.read_csv(entropy_file)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {subject} {session}: Load error - {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Merge keys\n",
    "    merge_keys = ['subject', 'session', 'task', 'trial_idx']\n",
    "    \n",
    "    # Check if merge keys exist\n",
    "    missing_keys = [k for k in merge_keys if k not in df_complete.columns or k not in df_entropy.columns]\n",
    "    if missing_keys:\n",
    "        print(f\"✗ {subject} {session}: Missing keys {missing_keys}\")\n",
    "        continue\n",
    "    \n",
    "    # Merge\n",
    "    df_merged = df_complete.merge(\n",
    "        df_entropy,\n",
    "        on=merge_keys,\n",
    "        how='inner',\n",
    "        suffixes=('', '_entropy')\n",
    "    )\n",
    "    \n",
    "    # Remove duplicate columns (keep first occurrence)\n",
    "    # Columns like event_code may appear in both\n",
    "    dup_cols = [c for c in df_merged.columns if c.endswith('_entropy')]\n",
    "    df_merged = df_merged.drop(columns=dup_cols, errors='ignore')\n",
    "    \n",
    "    # Count features\n",
    "    metadata_cols = ['subject', 'session', 'task', 'trial_idx', 'event_code', \n",
    "                     'onset_sample', 'onset_time']\n",
    "    feature_cols = [c for c in df_merged.columns if c not in metadata_cols]\n",
    "    \n",
    "    n_complete = len([c for c in df_complete.columns if c not in metadata_cols])\n",
    "    n_entropy = len([c for c in df_entropy.columns if c not in metadata_cols])\n",
    "    n_total = len(feature_cols)\n",
    "    \n",
    "    # Check for NaN\n",
    "    nan_pct = (df_merged[feature_cols].isna().sum().sum() / \n",
    "               (len(df_merged) * len(feature_cols))) * 100\n",
    "    \n",
    "    # Save merged file\n",
    "    output_file = OUTPUT_DIR / f\"{subject}_{session}_COMPLETE_V4.csv\"\n",
    "    df_merged.to_csv(output_file, index=False)\n",
    "    \n",
    "    merged_count += 1\n",
    "    total_trials += len(df_merged)\n",
    "    feature_stats.append(n_total)\n",
    "    \n",
    "    print(f\"✓ {subject} {session}: {len(df_merged):5d} trials | \" +\n",
    "          f\"Complete: {n_complete:3d} + Entropy: {n_entropy:3d} = {n_total:3d} total | \" +\n",
    "          f\"NaN: {nan_pct:.2f}%\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sessions merged:          {merged_count}\")\n",
    "print(f\"Total trials:             {total_trials:,}\")\n",
    "if feature_stats:\n",
    "    print(f\"Average features/session: {np.mean(feature_stats):.0f}\")\n",
    "    print(f\"Min features:             {np.min(feature_stats)}\")\n",
    "    print(f\"Max features:             {np.max(feature_stats)}\")\n",
    "print(f\"\\nOutput: {OUTPUT_DIR.resolve()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Quality check\n",
    "if merged_count > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUALITY CHECK (Sample File)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_file = list(OUTPUT_DIR.glob(\"*_COMPLETE_V4.csv\"))[0]\n",
    "    df_sample = pd.read_csv(sample_file)\n",
    "    \n",
    "    print(f\"File: {sample_file.name}\")\n",
    "    print(f\"Total columns:    {len(df_sample.columns)}\")\n",
    "    print(f\"Trials:           {len(df_sample)}\")\n",
    "    \n",
    "    metadata_cols = ['subject', 'session', 'task', 'trial_idx', 'event_code', \n",
    "                     'onset_sample', 'onset_time']\n",
    "    feature_cols = [c for c in df_sample.columns if c not in metadata_cols]\n",
    "    print(f\"Feature columns:  {len(feature_cols)}\")\n",
    "    \n",
    "    # NaN check\n",
    "    nan_pct = (df_sample[feature_cols].isna().sum().sum() / \n",
    "               (len(df_sample) * len(feature_cols))) * 100\n",
    "    print(f\"NaN percentage:   {nan_pct:.2f}%\")\n",
    "    \n",
    "    # Feature categories\n",
    "    print(f\"\\nFeature Categories:\")\n",
    "    print(f\"  Original (f0-f101):           {sum([1 for c in feature_cols if c.startswith('f') and c[1:].split('_')[0].isdigit()])}\")\n",
    "    print(f\"  Bandpowers (bp_):             {sum([1 for c in feature_cols if 'bp_' in c])}\")\n",
    "    print(f\"  Ratios (ratio_):              {sum([1 for c in feature_cols if c.startswith('ratio_')])}\")\n",
    "    print(f\"  PAC:                          {sum([1 for c in feature_cols if 'pac_' in c])}\")\n",
    "    print(f\"  LZ Complexity:                {sum([1 for c in feature_cols if 'lz_' in c])}\")\n",
    "    print(f\"  Permutation Entropy (pe_):    {sum([1 for c in feature_cols if c.startswith('pe_') and 'wpe_' not in c])}\")\n",
    "    print(f\"  Weighted PE (wpe_):           {sum([1 for c in feature_cols if c.startswith('wpe_')])}\")\n",
    "    print(f\"  Multiscale Entropy (mse_):    {sum([1 for c in feature_cols if 'mse_' in c])}\")\n",
    "    print(f\"  Mutual Information (mi_):     {sum([1 for c in feature_cols if c.startswith('mi_')])}\")\n",
    "    print(f\"  Cross-Sample Entropy:         {sum([1 for c in feature_cols if 'xsampen_' in c])}\")\n",
    "    print(f\"  Transfer Entropy (te_):       {sum([1 for c in feature_cols if c.startswith('te_') and 'te_' in c])}\")\n",
    "    print(f\"  Rolling Variance:             {sum([1 for c in feature_cols if 'var_' in c or 'cv_' in c])}\")\n",
    "    print(f\"  Asymmetry features:           {sum([1 for c in feature_cols if 'asym' in c])}\")\n",
    "    print(f\"  Change scores:                {sum([1 for c in feature_cols if '_change_' in c])}\")\n",
    "    \n",
    "    # Sample rows\n",
    "    print(f\"\\nSample Data (first trial):\")\n",
    "    print(df_sample[metadata_cols].head(1).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ READY FOR PHASE 1 & 2 LAB ANALYSIS\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e631940-831d-459f-a0ed-2807e6af22e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
