{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1543d06-8ede-47bc-ab99-ffa24393abae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process 48 files (should be 48):\n",
      "  sub-01_ses-S1_6STATES_v6_0.csv\n",
      "  sub-01_ses-S2_6STATES_v6_0.csv\n",
      "  sub-01_ses-S3_6STATES_v6_0.csv\n",
      "  sub-02_ses-S1_6STATES_v6_0.csv\n",
      "  sub-02_ses-S2_6STATES_v6_0.csv\n",
      "  sub-02_ses-S3_6STATES_v6_0.csv\n",
      "  sub-03_ses-S1_6STATES_v6_0.csv\n",
      "  sub-03_ses-S2_6STATES_v6_0.csv\n",
      "  sub-03_ses-S3_6STATES_v6_0.csv\n",
      "  sub-04_ses-S1_6STATES_v6_0.csv\n",
      "  sub-04_ses-S2_6STATES_v6_0.csv\n",
      "  sub-04_ses-S3_6STATES_v6_0.csv\n",
      "  sub-05_ses-S1_6STATES_v6_0.csv\n",
      "  sub-05_ses-S2_6STATES_v6_0.csv\n",
      "  sub-05_ses-S3_6STATES_v6_0.csv\n",
      "  sub-06_ses-S1_6STATES_v6_0.csv\n",
      "  sub-06_ses-S2_6STATES_v6_0.csv\n",
      "  sub-06_ses-S3_6STATES_v6_0.csv\n",
      "  sub-07_ses-S1_6STATES_v6_0.csv\n",
      "  sub-07_ses-S2_6STATES_v6_0.csv\n",
      "  sub-07_ses-S3_6STATES_v6_0.csv\n",
      "  sub-08_ses-S1_6STATES_v6_0.csv\n",
      "  sub-08_ses-S2_6STATES_v6_0.csv\n",
      "  sub-08_ses-S3_6STATES_v6_0.csv\n",
      "  sub-09_ses-S1_6STATES_v6_0.csv\n",
      "  sub-09_ses-S2_6STATES_v6_0.csv\n",
      "  sub-09_ses-S3_6STATES_v6_0.csv\n",
      "  sub-10_ses-S1_6STATES_v6_0.csv\n",
      "  sub-10_ses-S2_6STATES_v6_0.csv\n",
      "  sub-10_ses-S3_6STATES_v6_0.csv\n",
      "  sub-11_ses-S1_6STATES_v6_0.csv\n",
      "  sub-11_ses-S2_6STATES_v6_0.csv\n",
      "  sub-11_ses-S3_6STATES_v6_0.csv\n",
      "  sub-12_ses-S1_6STATES_v6_0.csv\n",
      "  sub-12_ses-S2_6STATES_v6_0.csv\n",
      "  sub-12_ses-S3_6STATES_v6_0.csv\n",
      "  sub-13_ses-S1_6STATES_v6_0.csv\n",
      "  sub-13_ses-S2_6STATES_v6_0.csv\n",
      "  sub-13_ses-S3_6STATES_v6_0.csv\n",
      "  sub-14_ses-S1_6STATES_v6_0.csv\n",
      "  sub-14_ses-S2_6STATES_v6_0.csv\n",
      "  sub-14_ses-S3_6STATES_v6_0.csv\n",
      "  sub-15_ses-S1_6STATES_v6_0.csv\n",
      "  sub-15_ses-S2_6STATES_v6_0.csv\n",
      "  sub-15_ses-S3_6STATES_v6_0.csv\n",
      "  sub-16_ses-S1_6STATES_v6_0.csv\n",
      "  sub-16_ses-S2_6STATES_v6_0.csv\n",
      "  sub-16_ses-S3_6STATES_v6_0.csv\n",
      "[1/48] sub-01_ses-S1_6STATES_v6_0.csv\n",
      "[2/48] sub-01_ses-S2_6STATES_v6_0.csv\n",
      "[3/48] sub-01_ses-S3_6STATES_v6_0.csv\n",
      "[4/48] sub-02_ses-S1_6STATES_v6_0.csv\n",
      "[5/48] sub-02_ses-S2_6STATES_v6_0.csv\n",
      "[6/48] sub-02_ses-S3_6STATES_v6_0.csv\n",
      "[7/48] sub-03_ses-S1_6STATES_v6_0.csv\n",
      "[8/48] sub-03_ses-S2_6STATES_v6_0.csv\n",
      "[9/48] sub-03_ses-S3_6STATES_v6_0.csv\n",
      "[10/48] sub-04_ses-S1_6STATES_v6_0.csv\n",
      "[11/48] sub-04_ses-S2_6STATES_v6_0.csv\n",
      "[12/48] sub-04_ses-S3_6STATES_v6_0.csv\n",
      "[13/48] sub-05_ses-S1_6STATES_v6_0.csv\n",
      "[14/48] sub-05_ses-S2_6STATES_v6_0.csv\n",
      "[15/48] sub-05_ses-S3_6STATES_v6_0.csv\n",
      "[16/48] sub-06_ses-S1_6STATES_v6_0.csv\n",
      "[17/48] sub-06_ses-S2_6STATES_v6_0.csv\n",
      "[18/48] sub-06_ses-S3_6STATES_v6_0.csv\n",
      "[19/48] sub-07_ses-S1_6STATES_v6_0.csv\n",
      "[20/48] sub-07_ses-S2_6STATES_v6_0.csv\n",
      "[21/48] sub-07_ses-S3_6STATES_v6_0.csv\n",
      "[22/48] sub-08_ses-S1_6STATES_v6_0.csv\n",
      "[23/48] sub-08_ses-S2_6STATES_v6_0.csv\n",
      "[24/48] sub-08_ses-S3_6STATES_v6_0.csv\n",
      "[25/48] sub-09_ses-S1_6STATES_v6_0.csv\n",
      "[26/48] sub-09_ses-S2_6STATES_v6_0.csv\n",
      "[27/48] sub-09_ses-S3_6STATES_v6_0.csv\n",
      "[28/48] sub-10_ses-S1_6STATES_v6_0.csv\n",
      "[29/48] sub-10_ses-S2_6STATES_v6_0.csv\n",
      "[30/48] sub-10_ses-S3_6STATES_v6_0.csv\n",
      "[31/48] sub-11_ses-S1_6STATES_v6_0.csv\n",
      "[32/48] sub-11_ses-S2_6STATES_v6_0.csv\n",
      "[33/48] sub-11_ses-S3_6STATES_v6_0.csv\n",
      "[34/48] sub-12_ses-S1_6STATES_v6_0.csv\n",
      "[35/48] sub-12_ses-S2_6STATES_v6_0.csv\n",
      "[36/48] sub-12_ses-S3_6STATES_v6_0.csv\n",
      "[37/48] sub-13_ses-S1_6STATES_v6_0.csv\n",
      "[38/48] sub-13_ses-S2_6STATES_v6_0.csv\n",
      "[39/48] sub-13_ses-S3_6STATES_v6_0.csv\n",
      "[40/48] sub-14_ses-S1_6STATES_v6_0.csv\n",
      "[41/48] sub-14_ses-S2_6STATES_v6_0.csv\n",
      "[42/48] sub-14_ses-S3_6STATES_v6_0.csv\n",
      "[43/48] sub-15_ses-S1_6STATES_v6_0.csv\n",
      "[44/48] sub-15_ses-S2_6STATES_v6_0.csv\n",
      "[45/48] sub-15_ses-S3_6STATES_v6_0.csv\n",
      "[46/48] sub-16_ses-S1_6STATES_v6_0.csv\n",
      "[47/48] sub-16_ses-S2_6STATES_v6_0.csv\n",
      "[48/48] sub-16_ses-S3_6STATES_v6_0.csv\n",
      "\n",
      "Saved combined session summary:\n",
      "C:\\Users\\rapol\\Downloads\\drift_analysis_results\\drift_analysis_results_phase2_session_summary.csv\n",
      "Rows: 48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "data_dir = Path(r\"C:\\Users\\rapol\\Downloads\\lab_analysis_v6_0_grounded\")\n",
    "output_dir = Path(r\"C:\\Users\\rapol\\Downloads\\drift_analysis_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# build first 48 files = subjects 01–16, sessions S1–S3\n",
    "subjects = [f\"{i:02d}\" for i in range(1, 17)]  # 01..16\n",
    "sessions = [\"S1\", \"S2\", \"S3\"]\n",
    "\n",
    "files = [\n",
    "    str(data_dir / f\"sub-{sub}_ses-{ses}_6STATES_v6_0.csv\")\n",
    "    for sub in subjects for ses in sessions\n",
    "]\n",
    "\n",
    "print(f\"Will process {len(files)} files (should be 48):\")\n",
    "for f in files:\n",
    "    print(\" \", Path(f).name)\n",
    "\n",
    "# ============= CONFIG FROM ANALYZER =============\n",
    "VALIDATED_Z_SCORES = [\n",
    "    'theta', 'beta', 'theta_beta_ratio', 'alpha', 'pe',\n",
    "    'delta', 'alpha_relative', 'gamma', 'mse', 'lz',\n",
    "    'pac', 'wpe', 'frontal_asymmetry', 'at_ratio'\n",
    "]\n",
    "\n",
    "MW_THRESHOLDS = {\n",
    "    'tbr_moderate': 0.25,\n",
    "    'alpha_decrease': -0.15,\n",
    "    'pe_moderate': -0.2,\n",
    "    'lz_decrease': -0.3,\n",
    "}\n",
    "\n",
    "FATIGUE_THRESHOLDS = {\n",
    "    'alpha_elevation': 0.8,\n",
    "    'delta_elevation': 0.3,\n",
    "    'theta_elevation': 0.2,\n",
    "    'beta_decrease': -0.3,\n",
    "    'pwr_loss': -0.4,\n",
    "}\n",
    "\n",
    "OVERLOAD_THRESHOLDS = {\n",
    "    'theta_extreme': 2.0,\n",
    "    'pac_surge': 1.2,\n",
    "}\n",
    "\n",
    "INTERVENTION_CUMULATIVE_DRIFT_THRESHOLD = 50.0\n",
    "INTERVENTION_COOLDOWN_TRIALS = 30\n",
    "OPTIMAL_STATES = ['Optimal-Monitoring', 'Optimal-Engaged']\n",
    "DRIFT_STATES = ['Mind-Wandering', 'Fatigue', 'Overload']\n",
    "\n",
    "# ============= CORE CLASSES/FUNCTIONS (minimal) =============\n",
    "class TemporalSmoothingEngine:\n",
    "    def __init__(self, window_size=10):\n",
    "        self.state_history = deque(maxlen=window_size)\n",
    "        self.drift_trajectory = deque(maxlen=20)\n",
    "    def add_trial(self, state, drift_strength):\n",
    "        self.state_history.append(state)\n",
    "        self.drift_trajectory.append(drift_strength)\n",
    "    def predict_drift_risk(self):\n",
    "        if len(self.drift_trajectory) < 3:\n",
    "            return 0\n",
    "        recent = list(self.drift_trajectory)[-10:]\n",
    "        mean_drift = np.mean(recent)\n",
    "        trend = recent[-1] - recent[0] if len(recent) >= 2 else 0\n",
    "        risk = min(100, int(mean_drift + trend*5))\n",
    "        return max(0, risk)\n",
    "\n",
    "class CumulativeDriftTracker:\n",
    "    def __init__(self, window_seconds=120):\n",
    "        self.drift_history = deque(maxlen=int(window_seconds/2))\n",
    "    def add_trial(self, is_drift):\n",
    "        self.drift_history.append(1 if is_drift else 0)\n",
    "    def get_cumulative_drift_pct(self):\n",
    "        if not self.drift_history:\n",
    "            return 0.0\n",
    "        return sum(self.drift_history)/len(self.drift_history)*100.0\n",
    "\n",
    "class WindowedStateClassifier:\n",
    "    def __init__(self, window_size=15):\n",
    "        self.instant_states = deque(maxlen=window_size)\n",
    "        self.z_scores_list = deque(maxlen=window_size)\n",
    "    def add_instant_classification(self, state, z_scores, signal_quality):\n",
    "        self.instant_states.append(state)\n",
    "        if z_scores:\n",
    "            self.z_scores_list.append(z_scores)\n",
    "    def get_windowed_state(self):\n",
    "        if not self.instant_states:\n",
    "            return 'Calibrating', 0.0, None\n",
    "        counts = {}\n",
    "        for s in self.instant_states:\n",
    "            counts[s] = counts.get(s, 0) + 1\n",
    "        majority_state = max(counts, key=counts.get)\n",
    "        confidence = counts[majority_state]/len(self.instant_states)\n",
    "        avg_z = None\n",
    "        if self.z_scores_list:\n",
    "            avg_z = {\n",
    "                key: np.mean([z.get(key, 0) for z in self.z_scores_list])\n",
    "                for key in self.z_scores_list[0].keys()\n",
    "            }\n",
    "        return majority_state, confidence, avg_z\n",
    "\n",
    "def classify_state(z_scores):\n",
    "    if not z_scores:\n",
    "        return 'Calibrating'\n",
    "    tbr = z_scores.get('theta_beta_ratio', 0)\n",
    "    alpha = z_scores.get('alpha', 0)\n",
    "    theta = z_scores.get('theta', 0)\n",
    "    delta = z_scores.get('delta', 0)\n",
    "    if theta > 1.5 and tbr > 0.8:\n",
    "        return 'Overload'\n",
    "    if alpha > 0.5 and delta > 0.2 and tbr < 0.1:\n",
    "        return 'Fatigue'\n",
    "    if tbr > 0.3 and alpha < -0.2:\n",
    "        return 'Mind-Wandering'\n",
    "    if theta > 0.3 and tbr > 0.1 and alpha > -0.2:\n",
    "        return 'Optimal-Engaged'\n",
    "    return 'Optimal-Monitoring'\n",
    "\n",
    "def detect_drift_enhanced(z_scores, windowed_state):\n",
    "    if not z_scores:\n",
    "        return {'drift_strength': 0, 'drift_label': 'NONE',\n",
    "                'drift_markers': [], 'error_risk': 0}\n",
    "    markers = []\n",
    "    strength = 0\n",
    "    if z_scores.get('theta_beta_ratio', 0) > MW_THRESHOLDS['tbr_moderate']:\n",
    "        markers.append('high_TBR'); strength += 15\n",
    "    if z_scores.get('alpha', 0) < MW_THRESHOLDS['alpha_decrease']:\n",
    "        markers.append('low_alpha'); strength += 15\n",
    "    if z_scores.get('pe', 0) < MW_THRESHOLDS['pe_moderate']:\n",
    "        markers.append('low_PE'); strength += 10\n",
    "    if z_scores.get('lz', 0) < MW_THRESHOLDS['lz_decrease']:\n",
    "        markers.append('low_LZ'); strength += 10\n",
    "    if z_scores.get('alpha', 0) > FATIGUE_THRESHOLDS['alpha_elevation']:\n",
    "        markers.append('high_alpha'); strength += 15\n",
    "    if z_scores.get('delta', 0) > FATIGUE_THRESHOLDS['delta_elevation']:\n",
    "        markers.append('high_delta'); strength += 10\n",
    "    if z_scores.get('theta', 0) > FATIGUE_THRESHOLDS['theta_elevation']:\n",
    "        markers.append('high_theta'); strength += 8\n",
    "    if z_scores.get('beta', 0) < FATIGUE_THRESHOLDS['beta_decrease']:\n",
    "        markers.append('low_beta'); strength += 10\n",
    "    if z_scores.get('theta', 0) > OVERLOAD_THRESHOLDS['theta_extreme']:\n",
    "        markers.append('extreme_theta'); strength += 20\n",
    "    if z_scores.get('pac', 0) > OVERLOAD_THRESHOLDS['pac_surge']:\n",
    "        markers.append('pac_surge'); strength += 12\n",
    "\n",
    "    if len(markers) >= 2 and strength >= 25:\n",
    "        if any(m in markers for m in ['high_TBR', 'low_alpha', 'low_PE']):\n",
    "            drift_label = 'CONFIRMED'; error_risk = min(100, strength+20)\n",
    "        elif any(m in markers for m in ['high_alpha', 'high_delta']):\n",
    "            drift_label = 'MODERATE'; error_risk = min(100, strength+10)\n",
    "        elif any(m in markers for m in ['extreme_theta', 'pac_surge']):\n",
    "            drift_label = 'STRONG'; error_risk = min(100, strength+30)\n",
    "        else:\n",
    "            drift_label = 'WEAK'; error_risk = min(100, strength)\n",
    "    elif len(markers) >= 1 and strength >= 15:\n",
    "        drift_label = 'WEAK'; error_risk = min(100, strength)\n",
    "    else:\n",
    "        drift_label = 'NONE'; error_risk = 0\n",
    "\n",
    "    return {\n",
    "        'drift_strength': min(100, strength),\n",
    "        'drift_label': drift_label,\n",
    "        'drift_markers': markers,\n",
    "        'error_risk': error_risk,\n",
    "    }\n",
    "\n",
    "class RetrospectiveDriftAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.temporal_smoother = TemporalSmoothingEngine()\n",
    "        self.windowed_classifier = WindowedStateClassifier()\n",
    "        self.cumulative_drift_tracker = CumulativeDriftTracker()\n",
    "        self.last_intervention_trial = -INTERVENTION_COOLDOWN_TRIALS\n",
    "        self.trial_count = 0\n",
    "    def process_trial(self, z_scores, signal_quality=0.7):\n",
    "        self.trial_count += 1\n",
    "        instant_state = classify_state(z_scores)\n",
    "        self.windowed_classifier.add_instant_classification(instant_state, z_scores, signal_quality)\n",
    "        windowed_state, confidence, avg_z = self.windowed_classifier.get_windowed_state()\n",
    "        use_z = avg_z if avg_z else z_scores\n",
    "        drift_info = detect_drift_enhanced(use_z, windowed_state)\n",
    "        self.temporal_smoother.add_trial(windowed_state, drift_info['drift_strength'])\n",
    "        drift_risk = self.temporal_smoother.predict_drift_risk()\n",
    "        is_drift = windowed_state in DRIFT_STATES\n",
    "        self.cumulative_drift_tracker.add_trial(is_drift)\n",
    "        cumulative_drift_pct = self.cumulative_drift_tracker.get_cumulative_drift_pct()\n",
    "\n",
    "        intervention_triggered = False\n",
    "        if (cumulative_drift_pct > INTERVENTION_CUMULATIVE_DRIFT_THRESHOLD and\n",
    "            self.trial_count - self.last_intervention_trial >= INTERVENTION_COOLDOWN_TRIALS):\n",
    "            intervention_triggered = True\n",
    "            self.last_intervention_trial = self.trial_count\n",
    "\n",
    "        out = {\n",
    "            'trial': self.trial_count,\n",
    "            'instant_state': instant_state,\n",
    "            'windowed_state': windowed_state,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'signal_quality': round(signal_quality, 2),\n",
    "            'drift_strength': drift_info['drift_strength'],\n",
    "            'drift_label': drift_info['drift_label'],\n",
    "            'drift_markers': ','.join(drift_info['drift_markers']),\n",
    "            'error_risk': drift_info['error_risk'],\n",
    "            'drift_risk': drift_risk,\n",
    "            'cumulative_drift_pct': round(cumulative_drift_pct, 1),\n",
    "            'intervention_triggered': 'YES' if intervention_triggered else 'NO',\n",
    "        }\n",
    "        for k, v in z_scores.items():\n",
    "            out[f'z_{k}'] = round(v, 3)\n",
    "        return out\n",
    "\n",
    "# ============= RUN OVER 48 FILES → ONE SESSION SUMMARY CSV =============\n",
    "all_session_stats = []\n",
    "\n",
    "for i, file_path in enumerate(files, 1):\n",
    "    fp = Path(file_path)\n",
    "    print(f\"[{i}/{len(files)}] {fp.name}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # group by subject/session in case a file has >1 subject/session\n",
    "    for (subj, sess), g in df.groupby(['subject', 'session']):\n",
    "        analyzer = RetrospectiveDriftAnalyzer()\n",
    "        session_results = []\n",
    "        for _, row in g.iterrows():\n",
    "            z_scores = {}\n",
    "            for feat in VALIDATED_Z_SCORES:\n",
    "                col = f'z_{feat}'\n",
    "                if col in row.index:\n",
    "                    z_scores[feat] = float(row[col])\n",
    "            signal_quality = float(row.get('signal_quality', 0.7))\n",
    "            out = analyzer.process_trial(z_scores, signal_quality)\n",
    "            out['subject'] = subj\n",
    "            out['session'] = sess\n",
    "            session_results.append(out)\n",
    "\n",
    "        df_sess = pd.DataFrame(session_results)\n",
    "        n_trials = len(df_sess)\n",
    "        n_interventions = (df_sess['intervention_triggered']=='YES').sum()\n",
    "        pct_optimal = (df_sess['windowed_state'].isin(OPTIMAL_STATES)).sum()/n_trials*100\n",
    "        pct_drift   = (df_sess['windowed_state'].isin(DRIFT_STATES)).sum()/n_trials*100\n",
    "        mean_cum_drift = df_sess['cumulative_drift_pct'].mean()\n",
    "        max_cum_drift  = df_sess['cumulative_drift_pct'].max()\n",
    "        mean_error_risk = df_sess['error_risk'].mean()\n",
    "\n",
    "        all_session_stats.append({\n",
    "            'subject': subj,\n",
    "            'session': sess,\n",
    "            'n_trials': n_trials,\n",
    "            'n_interventions': int(n_interventions),\n",
    "            'pct_optimal': round(pct_optimal, 1),\n",
    "            'pct_drift': round(pct_drift, 1),\n",
    "            'mean_cumulative_drift': round(mean_cum_drift, 1),\n",
    "            'max_cumulative_drift': round(max_cum_drift, 1),\n",
    "            'mean_error_risk': round(mean_error_risk, 1),\n",
    "        })\n",
    "\n",
    "# save ONE combined session-summary CSV with your requested name\n",
    "session_df = pd.DataFrame(all_session_stats)\n",
    "session_csv = output_dir / \"drift_analysis_results_phase2_session_summary.csv\"\n",
    "session_df.to_csv(session_csv, index=False)\n",
    "\n",
    "print(\"\\nSaved combined session summary:\")\n",
    "print(session_csv)\n",
    "print(f\"Rows: {len(session_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603a996-d8ee-45a6-a4d6-6eab210beaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
